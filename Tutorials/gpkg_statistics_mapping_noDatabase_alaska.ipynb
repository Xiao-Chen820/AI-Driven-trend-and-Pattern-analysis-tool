{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-5uUpahSql5"
      },
      "source": [
        "# 1. Environment set up #\n",
        "Import all the python packages needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvrWeoh8SERq"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "import os, uuid, shutil\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import rasterio, cv2\n",
        "from rasterio.transform import from_origin\n",
        "from rasterio.features import rasterize\n",
        "from skimage.morphology import skeletonize\n",
        "\n",
        "import morecantile\n",
        "from shapely.geometry import box, LineString, Polygon\n",
        "from pyproj import Transformer\n",
        "\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "import multiprocessing\n",
        "from joblib import Parallel, delayed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibduK3vxTETf"
      },
      "source": [
        "# 2. Data Fetch Set up #\n",
        "Build a robust HTTP session to catch different request issues, so that we won't miss any data when downloading data from Arctic Center and got varying issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mstN7HxSJWe"
      },
      "outputs": [],
      "source": [
        "\n",
        "SESSION = requests.Session()\n",
        "__retry = Retry(\n",
        "    total=5, backoff_factor=0.5,\n",
        "    status_forcelist=[429, 500, 502, 503, 504]\n",
        ")\n",
        "__adapter = HTTPAdapter(max_retries=__retry)\n",
        "SESSION.mount('https://', __adapter)\n",
        "__lock = multiprocessing.Manager().Lock()\n",
        "\n",
        "_stats_names = ['count', 'area', 'diameter', 'dia_min', 'dia_max', 'dia_median',\n",
        "                'perimeter', 'width', 'LCP_count', 'iwn_len']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TONeJ7qel5lu"
      },
      "source": [
        "# 3. Convert a grid into 256km * 256 km, with 1km as a pixel size #\n",
        "We have created 230 grids to cover the entire Pan-Arctic region.\n",
        "\n",
        "This seperates the study area into 230 grids, so we can process the grid as processing unit.\n",
        "\n",
        "Each grid contains 256 * 256 1km pixels. All of the IWP statistics are calculated amd aggregated within these 1km pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAm-GRkFTWHI"
      },
      "outputs": [],
      "source": [
        "N_PIXELS = 256\n",
        "SIZE_PIXEL = 1000    # 1km"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzTb-GV8SQBL"
      },
      "outputs": [],
      "source": [
        "def gen_pixel_bounds(cell_bounds):\n",
        "    minx, miny, maxx, maxy = cell_bounds\n",
        "    x = range(round(minx), round(maxx), SIZE_PIXEL)\n",
        "    y = range(round(miny), round(maxy), SIZE_PIXEL)\n",
        "    assert len(x) == N_PIXELS and len(y) == N_PIXELS, \"Invalid cell bounds.\"\n",
        "\n",
        "    return [(i, j, i + SIZE_PIXEL, j + SIZE_PIXEL) for j in y for i in x]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YaG_V96nI95"
      },
      "source": [
        "# 4. Get all tiles that intersect within the  grid #\n",
        "\n",
        "After getting the bounding box of a grid, we"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_P8ThllSSAu"
      },
      "outputs": [],
      "source": [
        "def get_intersected_tiles(bounds, tms, zoom=15):\n",
        "    # Define the transformer from EPSG:3413 to EPSG:4326\n",
        "    transformer = Transformer.from_crs(\"EPSG:3413\", \"EPSG:4326\")\n",
        "\n",
        "    # Create a polygon from the bounding box\n",
        "    polygon = box(*bounds)\n",
        "\n",
        "    # Define the interval for sampling points\n",
        "    interval = SIZE_PIXEL // 5\n",
        "\n",
        "    # Sample points along the edges of the polygon\n",
        "    points = []\n",
        "    for i in range(len(polygon.exterior.coords) - 1):\n",
        "        line = LineString([polygon.exterior.coords[i], polygon.exterior.coords[i + 1]])\n",
        "        num_points = int(line.length // interval)\n",
        "        points.extend([line.interpolate(float(j) / num_points, normalized=True) for j in range(num_points + 1)])\n",
        "\n",
        "    # Convert the points to a list of coordinates\n",
        "    coords = [(point.x, point.y) for point in points]\n",
        "\n",
        "    # Transform the coordinates\n",
        "    coords = [transformer.transform(x, y) for x, y in coords]\n",
        "    coords = [(y, x) for x, y in coords]\n",
        "\n",
        "    pixel_bbox = gpd.GeoDataFrame(geometry=[Polygon(coords)])\n",
        "\n",
        "    # get all tiles that intersect with the polygon\n",
        "    tiles = list(tms.tiles(*pixel_bbox.total_bounds, zooms=zoom))\n",
        "    bbox_func = lambda x: box(x.left, x.bottom, x.right, x.top)\n",
        "    filtered_tiles = [tile for tile in tiles if pixel_bbox.intersects(bbox_func(tms.bounds(tile))).any()]\n",
        "    # print(len(filtered_tiles))\n",
        "\n",
        "    return filtered_tiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6H3XRu3rfT4"
      },
      "source": [
        "# 5. Download all geopackages, if they are stored in remote #\n",
        "\n",
        "Download all geopackages if they fall within a pixel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbCCJT-oSVW7"
      },
      "outputs": [],
      "source": [
        "def download_tile(tile, download_root='downloads'):\n",
        "    url = f'https://arcticdata.io/data/10.18739/A2KW57K57/iwp_geopackage_high/WGS1984Quad/{tile.z}/{tile.x}/{tile.y}.gpkg'\n",
        "    download_path = os.path.join(download_root, f'{tile.z}_{tile.x}_{tile.y}.gpkg')\n",
        "\n",
        "    try:\n",
        "        # Send a HEAD request to check if the URL is available\n",
        "        response = requests.head(url)\n",
        "        if response.status_code == 200:\n",
        "            if int(response.headers['Content-Length']) > 1024**3:\n",
        "                raise RuntimeError('oversized source file detected.')\n",
        "            # URL is available, proceed to download\n",
        "            response = SESSION.get(url)\n",
        "            with open(download_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            return True\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error checking URL: {url}, Error: {e}\", file=open('dl_err.log', 'a'))\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yXukKfsr_Hs"
      },
      "source": [
        "# 6. Find the geopackages, if they are store in local #\n",
        "If the tiles have been stored in local, then working with these tiles directly without downloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rUIAymvSXcK"
      },
      "outputs": [],
      "source": [
        "def tiles_from_local(tile, local_dir='data', working_dir='downloads'):\n",
        "    local_path = os.path.join(local_dir, f'{tile.z}/{tile.x}/{tile.y}.gpkg')\n",
        "    download_path = os.path.join(working_dir, f'{tile.z}_{tile.x}_{tile.y}.gpkg')\n",
        "    if os.path.exists(local_path):\n",
        "        os.link(local_path, download_path)\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrtagLKZtUI9"
      },
      "source": [
        "# (Optional) 7. Ice Wedge Network Skeletonization #\n",
        "This algorithm is not included to generate any of the final data products so far, as it needs the validation from Elias's code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40WNy8rzSamS"
      },
      "outputs": [],
      "source": [
        "def IWP_skelenize(geoms, bounds, size=SIZE_PIXEL, kernel_size=11):\n",
        "    xmin, _, _, ymax = bounds\n",
        "    IWP_raster = rasterize(\n",
        "        [(geom, 1) for geom in geoms],\n",
        "        out_shape=(size, size),\n",
        "        transform=from_origin(xmin, ymax, 1, 1),\n",
        "        fill=0, dtype=np.uint8\n",
        "    )\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
        "    IWP_dilated = cv2.dilate(IWP_raster, kernel, iterations=1)\n",
        "    difference_raster = IWP_dilated - IWP_raster\n",
        "    IWP_skeleton = skeletonize(difference_raster)\n",
        "\n",
        "    # from PIL import Image\n",
        "    # im = Image.fromarray(IWP_skeleton.astype(np.uint8) * 255)\n",
        "    # im.save('skeleton.png')\n",
        "    # print(bounds)\n",
        "\n",
        "    # with rasterio.open(\n",
        "    #     'skeleton.tif', 'w',\n",
        "    #     driver='GTiff',\n",
        "    #     height=size,\n",
        "    #     width=size,\n",
        "    #     count=1,\n",
        "    #     dtype=np.uint8,\n",
        "    #     crs='EPSG:3413',\n",
        "    #     transform=from_origin(xmin, ymax, 1, 1),\n",
        "    # ) as dst:\n",
        "    #     dst.write(IWP_skeleton, 1)\n",
        "\n",
        "    return IWP_skeleton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW05AY03uU2V"
      },
      "source": [
        "# 8. Calculate the intersected geopackages within a pixel #\n",
        "Calculate statistics profile of deduplicated geopakcages within this 1km pixel:\n",
        "\n",
        "\n",
        "*   IWP count\n",
        "*   Area sum\n",
        "*   Length(diameter) sum/min/max/median/mean/std\n",
        "*   Perimeter sum\n",
        "*   Width sum\n",
        "*   LCP count\n",
        "\n",
        "However, if the file size of geopakcage is larger than 3GB, then we treated it as an outlier, and won't calculate it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDTE8rJoSdDt"
      },
      "outputs": [],
      "source": [
        "def data_analyse(tiles, bounds, file_root='downloads', crs='EPSG:3413'):\n",
        "    gdf = gpd.GeoDataFrame()\n",
        "    for tile in tiles:\n",
        "        tile_path = os.path.join(file_root, f'{tile.z}_{tile.x}_{tile.y}.gpkg')\n",
        "        if os.path.getsize(tile_path) > 1024**3:\n",
        "            with __lock:\n",
        "                _gdf = gpd.read_file(tile_path)\n",
        "                dedup_gdf = _gdf[_gdf['staging_duplicated'] == False]\n",
        "                del _gdf\n",
        "        else:\n",
        "            _gdf = gpd.read_file(tile_path)\n",
        "            dedup_gdf = _gdf[_gdf['staging_duplicated'] == False]\n",
        "        gdf = pd.concat([gdf, dedup_gdf], ignore_index=True)\n",
        "\n",
        "    # fileter the data based on the column 'centroidX' and 'centroidY' to get the data within the polygon\n",
        "    inbox_gdf = gdf[\n",
        "        gdf['CentroidX'].between(bounds[0], bounds[2]) &\n",
        "        gdf['CentroidY'].between(bounds[1], bounds[3])\n",
        "    ]\n",
        "\n",
        "    # get the skeleton of the IWP\n",
        "    inbox_gdf = inbox_gdf.to_crs(crs)\n",
        "    # IWP_skeleton = IWP_skelenize(inbox_gdf['geometry'], bounds)\n",
        "\n",
        "    # print(inbox_gdf.columns)\n",
        "    stats = [\n",
        "        len(inbox_gdf),\n",
        "        inbox_gdf['Area'].sum(),\n",
        "        inbox_gdf['Length'].sum(),\n",
        "        inbox_gdf['Length'].min(),\n",
        "        inbox_gdf['Length'].max(),\n",
        "        inbox_gdf['Length'].median(),\n",
        "        inbox_gdf['Length'].mean(),\n",
        "        inbox_gdf['Length'].std(),\n",
        "        inbox_gdf['Perimeter'].sum(),\n",
        "        inbox_gdf['Width'].sum(),\n",
        "        (inbox_gdf['Class'].astype(int) == 1).sum(),\n",
        "        # IWP_skeleton.sum()\n",
        "    ]\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8h7w4PMwh58"
      },
      "source": [
        "# 9. Start processing #\n",
        "Once we have the above steps set up, we can integrate them and start processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6tpRrOzSfQo"
      },
      "outputs": [],
      "source": [
        "def process_pixel(index, pixel_bounds, tms, zoom=15, remote=True):\n",
        "    # generate uuid for the process\n",
        "    process_uuid = str(uuid.uuid4())\n",
        "    dl_root = '.' + process_uuid\n",
        "    os.makedirs(dl_root)\n",
        "\n",
        "    stats = [0.] * len(_stats_names)\n",
        "    try:\n",
        "        # Get the intersected tiles\n",
        "        tiles = get_intersected_tiles(pixel_bounds, tms, zoom)\n",
        "        # Download the tiles if remote\n",
        "        if remote:\n",
        "            downloaded_tiles = [tile for tile in tiles\n",
        "                                if download_tile(tile, download_root=dl_root)]\n",
        "        # Get the tiles directly if local\n",
        "        else:\n",
        "            downloaded_tiles = [tile for tile in tiles\n",
        "                                if tiles_from_local(tile, local_dir='data', working_dir=dl_root)]\n",
        "        # Analyse the data\n",
        "        if len(downloaded_tiles) > 0:\n",
        "            stats = data_analyse(downloaded_tiles, pixel_bounds, file_root=dl_root)\n",
        "    # If the geopackage takes too long to download, we will mark it -99 as an outlier.\n",
        "    except RuntimeError:\n",
        "        stats = [-99.] * len(_stats_names)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing pixel {index}, Error: {e}\", file=open('proc_err.log', 'a'))\n",
        "    finally:\n",
        "        # Clean up\n",
        "        shutil.rmtree(dl_root)\n",
        "        return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJT5o-fzgYa"
      },
      "source": [
        "# 10. Save the stats matrix as a geotiff file #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BdekGaGSiM1"
      },
      "outputs": [],
      "source": [
        "def save_matrix_as_geotiff(matrix, cell_bounds, output_path, crs='EPSG:3413'):\n",
        "    height, width = matrix.shape\n",
        "    xmin, _, _, ymax = cell_bounds\n",
        "\n",
        "    with rasterio.open(\n",
        "        output_path,\n",
        "        'w',\n",
        "        driver='GTiff',\n",
        "        height=height,\n",
        "        width=width,\n",
        "        count=1,\n",
        "        dtype=matrix.dtype,\n",
        "        crs=crs,\n",
        "        transform=from_origin(round(xmin), round(ymax), SIZE_PIXEL, SIZE_PIXEL),\n",
        "    ) as dst:\n",
        "        dst.write(np.flipud(matrix), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvu6Ow0gzvGB"
      },
      "source": [
        "# 11. Wrap-up: Batch process pixels within a grid #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBrKJ2ZlJeuM"
      },
      "outputs": [],
      "source": [
        "def main(cell_index, n_workers=20):\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    tms = morecantile.tms.get(\"WGS1984Quad\")\n",
        "    grid = gpd.read_file(\"grid_230.geojson\")\n",
        "    cell = grid.loc[cell_index - 1, \"geometry\"]\n",
        "\n",
        "    pixel_bounds = gen_pixel_bounds(cell.bounds)\n",
        "    # ymin, xmin = -421744, -1811198\n",
        "    # pixel_bounds = [[xmin, ymin, xmin + SIZE_PIXEL, ymin + SIZE_PIXEL]]\n",
        "\n",
        "    # Process the pixel in parallel\n",
        "    mapper = Parallel(n_jobs=n_workers)\n",
        "    process = delayed(process_pixel)\n",
        "    results = mapper(process(i, pixel, tms) for i, pixel in enumerate(tqdm(pixel_bounds)))\n",
        "\n",
        "    # Save the results as a GeoTIFF\n",
        "    cell_name = f\"cell_{cell_index}\"\n",
        "    os.makedirs(cell_name, exist_ok=True)\n",
        "    np_results = np.array(results).reshape(N_PIXELS, N_PIXELS, len(_stats_names))\n",
        "    for i, name in enumerate(_stats_names):\n",
        "        # print (np_results[..., i])\n",
        "        save_matrix_as_geotiff(np_results[..., i], cell.bounds, f'{cell_name}/{cell_name}_{name}.tif')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if len(sys.argv) > 1:\n",
        "        main(int(sys.argv[1]), n_workers=12)\n",
        "    else:\n",
        "        exit(\"Please provide the cell index.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
